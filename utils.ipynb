{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd040fe7-1b10-4336-b376-3fd9d1d34a2c",
   "metadata": {
    "id": "cd040fe7-1b10-4336-b376-3fd9d1d34a2c"
   },
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc8822f-252a-4785-9a71-9c8ca3108edc",
   "metadata": {
    "id": "5dc8822f-252a-4785-9a71-9c8ca3108edc"
   },
   "outputs": [],
   "source": [
    "def plot_histogram(data_col, title, xlabel, ylabel, x_range=None):\n",
    "    \"\"\"\n",
    "    Plot histogram showing the distribution of patient ages.\n",
    "\n",
    "    Parameters:\n",
    "    data (Series): Pandas Series containing patient ages.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(data_col, bins=20, kde=True, color='skyblue')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    if x_range:\n",
    "        plt.xlim(x_range)\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59be6f4-4a88-4389-bc2a-cd794d1361f0",
   "metadata": {
    "id": "f59be6f4-4a88-4389-bc2a-cd794d1361f0"
   },
   "source": [
    "# Image Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592dd410-f1f8-421e-96a8-1a7315ac9512",
   "metadata": {
    "id": "592dd410-f1f8-421e-96a8-1a7315ac9512"
   },
   "outputs": [],
   "source": [
    "def forresnet(image_sample):\n",
    "    \"\"\"\n",
    "    Preprocess the image for ResNet-50 model\n",
    "\n",
    "    \"\"\"\n",
    "    # Assuming images[0] is a SimpleITK image\n",
    "    image_sitk = image_sample\n",
    "\n",
    "\n",
    "    # # Selecting a single slice (e.g., the first slice)\n",
    "    # single_slice = image_sitk[0, :, :]\n",
    "\n",
    "    # Convert SimpleITK image to numpy array\n",
    "    image_array = sitk.GetArrayFromImage(image_sitk)  #single_slice\n",
    "    slice_index = 0\n",
    "    image_array = image_array[slice_index, :, :]\n",
    "    # print(\"Original shape:\", image_array.shape)  # (23, 384)\n",
    "\n",
    "    # Reshape to add an extra dimension\n",
    "    image_array = np.expand_dims(image_array, axis=-1)\n",
    "    # print(\"Shape after adding dimension:\", image_array.shape)  # (23, 384, 1)\n",
    "\n",
    "    # Resize the image to (224, 224)\n",
    "    image_array = tf.image.resize(image_array, (224, 224))\n",
    "    # print(\"Resized shape:\", image_array.shape)  # (224, 224, 1)\n",
    "\n",
    "    # Remove the extra dimension\n",
    "    image_array = tf.squeeze(image_array, axis=-1)\n",
    "    # print(\"Shape after squeezing:\", image_array.shape)  # (224, 224)\n",
    "\n",
    "    # Convert to grayscale NOT NEEDED BECAUSE IT IS ALREADY IN GRAYSCALE\n",
    "    # image_array = tf.image.rgb_to_grayscale(image_array)\n",
    "    # print(\"Shape after converting to grayscale:\", image_array.shape)  # (224, 224, 1)\n",
    "\n",
    "    # Stack to create a 3-channel image\n",
    "    image_array = tf.stack([image_array] * 3, axis=-1)\n",
    "    # print(\"Shape after stacking:\", image_array.shape)  # (224, 224, 3)\n",
    "\n",
    "    # Normalize the image\n",
    "    # max_value = tf.reduce_max(image_array)\n",
    "    # image_array = image_array / max_value\n",
    "    image_array = image_array / 255\n",
    "\n",
    "    # print(\"Shape after normalization:\", image_array.shape)\n",
    "\n",
    "    # # Add batch dimension\n",
    "    # image_array = tf.expand_dims(image_array, axis=0)\n",
    "    # print(\"Final shape with batch dimension:\", image_array.shape)\n",
    "    \n",
    "    # print(\"Minimum value:\", tf.reduce_min(image_array))\n",
    "    # print(\"Maximunm value:\", tf.reduce_max(image_array))\n",
    "    # print('-------------------------')\n",
    "    \n",
    "\n",
    "    return image_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d40914-20ad-4c18-9f7f-18acd3403d46",
   "metadata": {
    "id": "18d40914-20ad-4c18-9f7f-18acd3403d46"
   },
   "source": [
    "# Model Evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8e5e30-758a-4a5e-b8a6-78a2fc6b35f4",
   "metadata": {
    "id": "4a8e5e30-758a-4a5e-b8a6-78a2fc6b35f4"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, threshold, X_train, y_train, X_val, y_val, X_test=None, y_test=None, include_test=False):\n",
    "    def calculate_metrics(X, y):\n",
    "        # Predict\n",
    "        y_pred = model.predict(X)\n",
    "        y_pred_binary = (y_pred > threshold).astype('int32')  # Convert probabilities to binary predictions\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y, y_pred_binary)\n",
    "        precision = precision_score(y, y_pred_binary, zero_division=0)\n",
    "        recall = recall_score(y, y_pred_binary)\n",
    "        f1 = f1_score(y, y_pred_binary)\n",
    "        conf_matrix = confusion_matrix(y, y_pred_binary)\n",
    "\n",
    "        # Find correctly and wrongly predicted indices\n",
    "        correct_predictions = [i for i in range(len(y)) if y_pred_binary[i] == y[i]]\n",
    "        wrong_predictions = [i for i in range(len(y)) if y_pred_binary[i] != y[i]]\n",
    "\n",
    "        return accuracy, precision, recall, f1, conf_matrix, correct_predictions, wrong_predictions\n",
    "\n",
    "    # Calculate metrics for train and validation sets\n",
    "    train_metrics = calculate_metrics(X_train, y_train)\n",
    "    val_metrics = calculate_metrics(X_val, y_val)\n",
    "\n",
    "    # If include_test is True, calculate metrics for the test set\n",
    "    test_metrics = None\n",
    "    if include_test:\n",
    "        test_metrics = calculate_metrics(X_test, y_test)\n",
    "\n",
    "    # Create lists for correct and wrong predictions\n",
    "    correct_predictions = []\n",
    "    wrong_predictions = []\n",
    "\n",
    "    # Add train, validation, and test predictions to the lists\n",
    "    if train_metrics[-2] is not None:\n",
    "        correct_predictions.append(train_metrics[-2])\n",
    "        wrong_predictions.append(train_metrics[-1])\n",
    "\n",
    "    if val_metrics[-2] is not None:\n",
    "        correct_predictions.append(val_metrics[-2])\n",
    "        wrong_predictions.append(val_metrics[-1])\n",
    "\n",
    "    if test_metrics[-2] is not None:\n",
    "        correct_predictions.append(test_metrics[-2])\n",
    "        wrong_predictions.append(test_metrics[-1])\n",
    "\n",
    "    # Create a DataFrame for metrics\n",
    "    metrics_df = pd.DataFrame([train_metrics, val_metrics, test_metrics],\n",
    "                              columns=['Accuracy', 'Precision', 'Recall', 'F1 Score', 'Confusion Matrix',\n",
    "                                       'Correct Predictions', 'Wrong Predictions'])\n",
    "\n",
    "    # Add a column for set type\n",
    "    metrics_df['Set Type'] = ['Train', 'Validation', 'Test']\n",
    "    metrics_df = metrics_df.set_index('Set Type')\n",
    "\n",
    "    # Remove the confusion matrix, correct predictions, and wrong predictions columns from the DataFrame\n",
    "    metrics_df = metrics_df.drop(['Confusion Matrix', 'Correct Predictions', 'Wrong Predictions'], axis=1)\n",
    "\n",
    "    # Filter out the test row if include_test is False\n",
    "    if not include_test:\n",
    "        metrics_df = metrics_df.drop('Test', axis=0)\n",
    "\n",
    "    display(metrics_df)\n",
    "\n",
    "    ### Plot confusion matrices\n",
    "    if include_test == False:\n",
    "        conf_matrices = [train_metrics[4], val_metrics[4]]\n",
    "        titles = ['Train', 'Validation']\n",
    "        num_plots = 2\n",
    "    else:\n",
    "        conf_matrices = [train_metrics[4], val_metrics[4], test_metrics[4]]\n",
    "        titles = ['Train', 'Validation', 'Test']\n",
    "        num_plots = 3\n",
    "\n",
    "    fig, axes = plt.subplots(1, num_plots, figsize=(5 * num_plots, 5))\n",
    "\n",
    "    for i, (conf_matrix, title) in enumerate(zip(conf_matrices, titles)):\n",
    "        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', ax=axes[i])\n",
    "        axes[i].set_title(f'Confusion Matrix - {title}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return metrics_df, correct_predictions, wrong_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ceb1ed-1716-4dc1-b190-00a082bbe91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_cam(model, img_arrays, layer_name):\n",
    "\n",
    "    #gradient tape context to record the forward pass\n",
    "    grad_model = tf.keras.models.Model([model.inputs], [model.get_layer(layer_name).output, model.output])\n",
    "\n",
    "    #initialize a list to store CAMs for each image\n",
    "    cams = []\n",
    "\n",
    "    for img_array in img_arrays:\n",
    "        #get the last convolutional layer and the output layer of the model\n",
    "        with tf.GradientTape() as tape:\n",
    "            conv_output, predictions = grad_model(img_array)\n",
    "            # Get the index of the top prediction\n",
    "            class_index = np.argmax(predictions[0])\n",
    "            # Fetch the gradient of the top predicted class with respect to the output feature map of the last convolutional layer\n",
    "            gradient = tape.gradient(predictions[:, class_index], conv_output)\n",
    "\n",
    "        #global average pooling to get the weights of each feature map\n",
    "        weights = tf.reduce_mean(gradient, axis=(1, 2))\n",
    "\n",
    "        #get the output of the last convolutional layer\n",
    "        conv_output = conv_output[0]\n",
    "\n",
    "        #generate the class activation map\n",
    "        cam = np.ones(conv_output.shape[:2], dtype=np.float32)  # Ensure cam has only 2 dimensions\n",
    "\n",
    "        #reshape the weights to match the spatial dimensions of the convolutional layer output\n",
    "        weights = tf.reshape(weights, (-1, 1, 1))\n",
    "\n",
    "        #multiply each feature map by its corresponding weight and sum them up\n",
    "        for i, w in enumerate(weights):\n",
    "            cam += w * conv_output[:, :, i]\n",
    "\n",
    "        #normalize the CAM\n",
    "        cam = cv2.resize(cam.numpy(), (224, 224))\n",
    "        cam = np.maximum(cam, 0)\n",
    "        cam_max = cam.max()\n",
    "        if cam_max != 0:\n",
    "            cam = cam / cam_max\n",
    "            \n",
    "        cams.append(cam)\n",
    "\n",
    "    return cams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9df264-909b-4ecb-8a67-3e285c0ac3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_gradcam(model, layer_name, X_images, indices, which_set, y_train, y_val, y_test, num_images=5):\n",
    "    \n",
    "    correct_imgs = [X_images[i] for i in indices]  # Load images\n",
    "\n",
    "    if which_set == 'train':\n",
    "        correct_labels = ['cancer' if y_train[i] == 1 else 'no cancer' for i in indices]\n",
    "    elif which_set == 'val':\n",
    "        correct_labels = ['cancer' if y_val[i] == 1 else 'no cancer' for i in indices]\n",
    "    elif which_set == 'test':\n",
    "        correct_labels = ['cancer' if y_test[i] == 1 else 'no cancer' for i in indices]\n",
    "\n",
    "\n",
    "    # Expand dimensions for each image\n",
    "    img_arrays = [np.expand_dims(img, axis=0) for img in correct_imgs]\n",
    "\n",
    "    # Generate GradCAMs\n",
    "    cams = grad_cam(model, img_arrays, layer_name)\n",
    "\n",
    "    # Display the original images and their GradCAM overlays\n",
    "    fig, axes = plt.subplots(2, num_images, figsize=(20, 8))\n",
    "\n",
    "    for i, (img, cam, correct_label) in enumerate(zip(correct_imgs, cams, correct_labels)):\n",
    "        axes[0, i].imshow(img)\n",
    "        axes[0, i].axis('off')\n",
    "        axes[0, i].set_title('Original')\n",
    "\n",
    "        axes[1, i].imshow(img)\n",
    "        axes[1, i].imshow(cam, cmap='jet', alpha=0.5)\n",
    "        axes[1, i].axis('off')\n",
    "        axes[1, i].set_title(f'GradCAM Overlay (True: {correct_label})')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
